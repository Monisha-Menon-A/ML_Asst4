{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7fd6beb-432a-41be-9c4f-bb823ac140be",
   "metadata": {},
   "source": [
    "1. Loading and Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b988e19-d670-4112-bf28-699f15a111ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data  # Features\n",
    "y = data.target  # Labels\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98575036-7f33-4f50-94ad-c049fde3aad6",
   "metadata": {},
   "source": [
    "Preprocessing Steps:\n",
    "Handling Missing Values: The breast cancer dataset from sklearn is clean and does not contain any missing values. If there were missing values, we would use techniques like imputation (mean, median, or mode) or drop rows/columns with missing values.\n",
    "\n",
    "Feature Scaling: The features in this dataset have different scales. For example, some features might range from 0 to 1, while others might range from 0 to 1000. Scaling the features is important for algorithms like SVM, k-NN, and Logistic Regression, which are sensitive to the scale of the input data. We use StandardScaler to standardize the features to have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa20d76a-a32f-4898-a766-b8d5f7809a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562c7656-352a-4c37-98bf-8a2e52870df6",
   "metadata": {},
   "source": [
    "Justification: Feature scaling ensures that all features contribute equally to the result, preventing features with larger scales from dominating the model. This is particularly important for distance-based algorithms like k-NN and SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c4c50a-5364-4ad3-a048-1919ad1f0ad0",
   "metadata": {},
   "source": [
    "2. Classification Algorithm Implementation:\n",
    "\n",
    "    1.Logistic Regression\n",
    "Description: Logistic Regression is a linear model used for binary classification. It estimates the probability of a sample belonging to a particular class using the logistic function.\n",
    "\n",
    "Suitability: It is simple and interpretable, making it a good baseline model for binary classification tasks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d62070dc-e456-470c-9a72-8daf514bd3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "Accuracy: 0.9825, Precision: 0.9907, Recall: 0.9815, F1 Score: 0.9860\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Train the model\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Logistic Regression:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b939c407-06f4-4499-b7aa-ab6e2e9cac96",
   "metadata": {},
   "source": [
    "2. Decision Tree Classifier\n",
    "Description: A Decision Tree splits the data into subsets based on feature values, creating a tree-like structure to make predictions.\n",
    "\n",
    "Suitability: It is easy to interpret and can handle non-linear relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2a989fd-f54c-405b-9f0d-8643d43e2fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier:\n",
      "Accuracy: 0.9415, Precision: 0.9712, Recall: 0.9352, F1 Score: 0.9528\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Train the model\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Decision Tree Classifier:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a9ecdb-9123-479a-9f7d-3437d18e3501",
   "metadata": {},
   "source": [
    "3. Random Forest Classifier\n",
    "Description: Random Forest is an ensemble method that builds multiple decision trees and combines their predictions to improve accuracy and reduce overfitting.\n",
    "\n",
    "Suitability: It is robust to overfitting and performs well on high-dimensional datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a490402-775c-4f3d-89c7-1e515d9969cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier:\n",
      "Accuracy: 0.9708, Precision: 0.9640, Recall: 0.9907, F1 Score: 0.9772\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train the model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Random Forest Classifier:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743f05a2-9623-4668-852e-ce3c6b3c5e1b",
   "metadata": {},
   "source": [
    "4. Support Vector Machine (SVM)\n",
    "Description: SVM finds the optimal hyperplane that separates the classes with the maximum margin.\n",
    "\n",
    "Suitability: It is effective in high-dimensional spaces and works well for binary classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15693465-5ab9-4675-8c5a-81fe3d6ce748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine:\n",
      "Accuracy: 0.9766, Precision: 0.9815, Recall: 0.9815, F1 Score: 0.9815\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train the model\n",
    "svm = SVC(random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Support Vector Machine:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c9378a-6b23-4d3b-aa72-d66ae65c0537",
   "metadata": {},
   "source": [
    "5. k-Nearest Neighbors (k-NN)\n",
    "Description: k-NN classifies a sample based on the majority class among its k-nearest neighbors in the feature space.\n",
    "\n",
    "Suitability: It is simple and works well for small datasets with clear separation between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c958d977-b6a6-4498-9412-1e88af5a8b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-Nearest Neighbors:\n",
      "Accuracy: 0.9591, Precision: 0.9633, Recall: 0.9722, F1 Score: 0.9677\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Train the model\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"k-Nearest Neighbors:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42ee2b7-4f81-47f1-b41a-49136ed9c300",
   "metadata": {},
   "source": [
    "3. Model Comparison:\n",
    "\n",
    "Accuracy:\n",
    "Logistic Regression has the highest accuracy (0.9825), followed by SVM (0.9766) and Random Forest (0.9708).\n",
    "\n",
    "Decision Tree (0.9415) and k-NN (0.9591) have the lowest accuracy.\n",
    "\n",
    "Precision:\n",
    "Logistic Regression has the highest precision (0.9907), followed by SVM (0.9815) and Decision Tree (0.9712).\n",
    "\n",
    "Random Forest (0.9640) and k-NN (0.9633) have slightly lower precision.\n",
    "\n",
    "Recall:\n",
    "Random Forest has the highest recall (0.9907), followed by k-NN (0.9722) and Logistic Regression (0.9815).\n",
    "\n",
    "Decision Tree (0.9352) has the lowest recall.\n",
    "\n",
    "F1 Score:\n",
    "Logistic Regression has the highest F1 score (0.9860), followed by Random Forest (0.9772) and SVM (0.9815).\n",
    "\n",
    "Decision Tree (0.9528) and k-NN (0.9677) have the lowest F1 scores.\n",
    "\n",
    "\n",
    "Conclusion:\n",
    "Logistic Regression is the best-performing algorithm for this dataset, with the highest accuracy, precision, and F1 score.\n",
    "\n",
    "SVM and Random Forest are strong alternatives, with SVM being slightly better in precision and F1 score, and Random Forest excelling in recall.\n",
    "\n",
    "k-NN performs moderately but is not as strong as the top three algorithms.\n",
    "\n",
    "Decision Tree is the worst-performing algorithm, likely due to overfitting or its inability to generalize well on this dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d0699b-294d-4242-bdf1-8bda4439a571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914d5dab-9f2b-47e4-865f-0b2227155f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817cbc52-082f-491c-92db-fb4cb14832d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
