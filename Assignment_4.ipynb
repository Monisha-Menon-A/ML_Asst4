{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "392246c0-8862-49a5-8dae-8d1b38915b47",
   "metadata": {},
   "source": [
    "1. Loading and Preprocessing\n",
    "Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4b988e19-d670-4112-bf28-699f15a111ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data  # Features\n",
    "y = data.target  # Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e190c4-ff6e-40fc-8212-dd165dada8f8",
   "metadata": {},
   "source": [
    "Preprocessing Steps:\n",
    "Handling Missing Values: The breast cancer dataset from sklearn is clean and does not contain any missing values. If there were missing values, we would use techniques like imputation (mean, median, or mode) or drop rows/columns with missing values.\n",
    "\n",
    "Feature Scaling: The features in this dataset have different scales. Scaling is necessary for algorithms like SVM, k-NN, and Logistic Regression, which are sensitive to the magnitude of the features. We use StandardScaler to standardize the features to have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aa20d76a-a32f-4898-a766-b8d5f7809a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f81173-293c-4f2c-8783-a878f82d9511",
   "metadata": {},
   "source": [
    "Justification: Scaling ensures that all features contribute equally to the model's performance, preventing features with larger magnitudes from dominating those with smaller magnitudes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c4c50a-5364-4ad3-a048-1919ad1f0ad0",
   "metadata": {},
   "source": [
    "2. Classification Algorithm Implementation:\n",
    "\n",
    "    1.Logistic Regression\n",
    "Description: Logistic Regression is a linear model used for binary classification. It estimates the probability of a sample belonging to a particular class using the logistic function.\n",
    "\n",
    "Suitability: It is simple and interpretable, making it a good baseline model for binary classification tasks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d62070dc-e456-470c-9a72-8daf514bd3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9824561403508771\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train the model\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = log_reg.predict(X_test)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b939c407-06f4-4499-b7aa-ab6e2e9cac96",
   "metadata": {},
   "source": [
    "2. Decision Tree Classifier\n",
    "Description: A Decision Tree splits the data into subsets based on feature values, creating a tree-like structure to make predictions.\n",
    "\n",
    "Suitability: It is easy to interpret and can handle non-linear relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e2a989fd-f54c-405b-9f0d-8643d43e2fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.9415204678362573\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Train the model\n",
    "dtree = DecisionTreeClassifier(random_state=42)\n",
    "dtree.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = dtree.predict(X_test)\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a9ecdb-9123-479a-9f7d-3437d18e3501",
   "metadata": {},
   "source": [
    "3. Random Forest Classifier\n",
    "Description: Random Forest is an ensemble method that builds multiple decision trees and combines their predictions to improve accuracy and reduce overfitting.\n",
    "\n",
    "Suitability: It is robust to overfitting and performs well on high-dimensional datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9a490402-775c-4f3d-89c7-1e515d9969cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9707602339181286\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train the model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = rf.predict(X_test)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743f05a2-9623-4668-852e-ce3c6b3c5e1b",
   "metadata": {},
   "source": [
    "4. Support Vector Machine (SVM)\n",
    "Description: SVM finds the optimal hyperplane that separates the classes with the maximum margin.\n",
    "\n",
    "Suitability: It is effective in high-dimensional spaces and works well for binary classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "15693465-5ab9-4675-8c5a-81fe3d6ce748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.9766081871345029\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train the model\n",
    "svm = SVC(kernel='linear', random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = svm.predict(X_test)\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c9378a-6b23-4d3b-aa72-d66ae65c0537",
   "metadata": {},
   "source": [
    "5. k-Nearest Neighbors (k-NN)\n",
    "Description: k-NN classifies a sample based on the majority class among its k-nearest neighbors in the feature space.\n",
    "\n",
    "Suitability: It is simple and works well for small datasets with clear separation between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c958d977-b6a6-4498-9412-1e88af5a8b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-NN Accuracy: 0.9590643274853801\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Train the model\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"k-NN Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde3776e-9eb8-4853-a2b0-3da39e1fbffd",
   "metadata": {},
   "source": [
    "3. Model Comparison:\n",
    "\n",
    "Algorithm\tAccuracy:\n",
    "Logistic Regression - 0.9825\n",
    "Decision Tree - 0.9415\n",
    "Random Forest - 0.9708\n",
    "SVM - 0.9766\n",
    "k-NN - 0.9591\n",
    "Observations:\n",
    "Best Performing Algorithm: Logistic Regression.\n",
    "\n",
    "Worst Performing Algorithm: Decision Tree had the lowest accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5c5224-6ff0-4154-a66c-37c55318ebfe",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "Logistic Regression is the best-performing model for this dataset, achieving the highest accuracy of 98.25%. It is a simple yet powerful algorithm for binary classification tasks like this.\n",
    "\n",
    "SVM and Random Forest also performed very well, with accuracies of 97.66% and 97.08%, respectively. These models are suitable for datasets with complex relationships and high dimensionality.\n",
    "\n",
    "k-NN performed decently with an accuracy of 95.91%, but it is slightly less effective than the top three algorithms.\n",
    "\n",
    "Decision Tree performed the worst, with an accuracy of 94.15%, likely due to overfitting or suboptimal hyperparameters. However, it can be improved with proper tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d0699b-294d-4242-bdf1-8bda4439a571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914d5dab-9f2b-47e4-865f-0b2227155f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817cbc52-082f-491c-92db-fb4cb14832d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
